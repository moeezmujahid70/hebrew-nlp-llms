{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2KRttHw41Ig4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moeezmujahid70/hebrew-nlp-llms/blob/main/improved_%3E_Replace_modern_Hebrew_words_Workflow_11_07_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AI Enhance Outline Workflow**\n",
        "1. Please add your **Antropic API key** (you can get it [here](https://console.anthropic.com/settings/keys))\n",
        "2. Please add your **OpenAI API key** (you can get it [here](https://platform.openai.com/api-keys))\n",
        "3. run the '**Install Dependencies**'\n",
        "4. When done run the workflow bellow\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7vx1eSqmk9Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Add API Keys and Install Dependencies**\n",
        "!pip install -q -U google-genai\n",
        "!pip install -q --upgrade python-docx pandas\n",
        "!pip install anthropic\n",
        "\n",
        "#!pip install mammoth sqlite3-binaries\n",
        "# Install necessary dependencies for processing .doc files\n",
        "\n",
        "!pip install python-docx textract\n",
        "#!apt-get install -y antiword catdoc\n",
        "\n",
        "from google.colab import userdata\n",
        "antropic_api_key = userdata.get('antropic_api_key')\n",
        "grok_api_key = userdata.get('grok_api_key')\n",
        "openai_api_key = userdata.get('openai_api_key')"
      ],
      "metadata": {
        "id": "KTNx34N2tfYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82dff5be-3dc4-4e73-c668-c4b89cac4623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.57.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Collecting textract\n",
            "  Using cached textract-1.6.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "\u001b[33mWARNING: Ignoring version 1.6.5 of textract since it has invalid metadata:\n",
            "Requested textract from https://files.pythonhosted.org/packages/6b/3e/ac16b6bf28edf78296aea7d0cb416b49ed30282ac8c711662541015ee6f3/textract-1.6.5-py3-none-any.whl has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    extract-msg (<=0.29.*)\n",
            "                 ~~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Using cached textract-1.6.4.tar.gz (17 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import re\n",
        "import requests\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from docx import Document\n",
        "import io\n",
        "import os\n",
        "import sqlite3\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "import hashlib\n",
        "# import re\n",
        "import re as regex_module\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class HebrewMatch:\n",
        "    word: str\n",
        "    traditional_equivalent: str\n",
        "    confidence: float\n",
        "    context_before: str\n",
        "    context_after: str\n",
        "    position: Tuple[int, int]\n",
        "    source: str  # 'database' or 'grok' or 'context'\n",
        "\n",
        "@dataclass\n",
        "class ModernWordCategory:\n",
        "    \"\"\"Categories of modern Hebrew words\"\"\"\n",
        "    TRANSLATABLE = \"translatable\"          # Has traditional equivalent\n",
        "    UNTRANSLATABLE = \"untranslatable\"      # No traditional equivalent\n",
        "    CONTEXTUAL = \"contextual\"              # Depends on context\n",
        "    ALREADY_TRADITIONAL = \"traditional\"    # Already traditional Hebrew\n",
        "\n",
        "\n",
        "def find_forbidden_words(document_text: str, forbidden_list: list) -> list:\n",
        "    \"\"\"\n",
        "    Find forbidden Hebrew words/phrases in a document.\n",
        "\n",
        "    Args:\n",
        "        document_text: The text content to search through\n",
        "        forbidden_list: List of forbidden Hebrew words/phrases\n",
        "\n",
        "    Returns:\n",
        "        List of forbidden words found in the document\n",
        "    \"\"\"\n",
        "    found_words = []\n",
        "\n",
        "    for forbidden_word in forbidden_list:\n",
        "        if forbidden_word.strip() and forbidden_word in document_text:\n",
        "            if forbidden_word not in found_words:  # Avoid duplicates\n",
        "                found_words.append(forbidden_word)\n",
        "\n",
        "    return found_words\n",
        "\n",
        "\n",
        "\n",
        "def scan_uploaded_docx_files():\n",
        "\n",
        "    print(\"Please select HEBREW DOCX files to upload...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for filename, file_content in uploaded.items():\n",
        "        print(f\"\\nScanning file: {filename}\")\n",
        "\n",
        "        if not filename.lower().endswith('.docx'):\n",
        "            print(f\"Warning: {filename} is not a DOCX file. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Extract text from DOCX\n",
        "        try:\n",
        "        # Create a file-like object from the content\n",
        "          docx_file = io.BytesIO(file_content)\n",
        "          doc = Document(docx_file)\n",
        "\n",
        "          # Extract text from all paragraphs\n",
        "          full_text = []\n",
        "          for paragraph in doc.paragraphs:\n",
        "              full_text.append(paragraph.text)\n",
        "\n",
        "          return '\\n'.join(full_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from DOCX: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def apply_replacements_safely(text: str, matches: List[HebrewMatch], grok_suggestions: Dict[str, str]) -> Tuple[str, List[Dict]]:\n",
        "    \"\"\"\n",
        "    Apply replacements safely without creating duplicate brackets\n",
        "    \"\"\"\n",
        "    processed_text = text\n",
        "    replacement_log = []\n",
        "    replaced_words = set()  # Track words already replaced\n",
        "\n",
        "    # Sort matches by confidence (highest first) to prioritize better replacements\n",
        "    sorted_matches = sorted(matches, key=lambda x: x.confidence, reverse=True)\n",
        "\n",
        "    print(f\"Applying {len(sorted_matches)} database matches...\")\n",
        "\n",
        "    # Apply database matches first (higher priority)\n",
        "    for match in sorted_matches:\n",
        "        if match.confidence > 0.5:  # Only apply high-confidence replacements\n",
        "\n",
        "            # Skip if word already replaced\n",
        "            if match.word in replaced_words:\n",
        "                continue\n",
        "\n",
        "            # Create pattern that doesn't match already replaced words\n",
        "            pattern = r'\\b' + re.escape(match.word) + r'\\b'\n",
        "\n",
        "            # Check if the word exists and hasn't been replaced yet\n",
        "            if re.search(pattern, processed_text) and f\"{{{match.word}}}\" not in processed_text:\n",
        "                replacement = f\"{{{match.word}}}[{match.traditional_equivalent}]\"\n",
        "                processed_text = re.sub(pattern, replacement, processed_text)\n",
        "                replaced_words.add(match.word)\n",
        "\n",
        "                replacement_log.append({\n",
        "                    'word': match.word,\n",
        "                    'replacement': match.traditional_equivalent,\n",
        "                    'confidence': match.confidence,\n",
        "                    'source': match.source,\n",
        "                    'priority': 'database'\n",
        "                })\n",
        "                # print(f\"  âœ… Replaced: {match.word} â†’ {match.traditional_equivalent}\")\n",
        "\n",
        "    print(f\"Applying {len(grok_suggestions)} Grok suggestions...\")\n",
        "\n",
        "    # Apply Grok suggestions (for words not already replaced)\n",
        "    for modern_word, traditional_word in grok_suggestions.items():\n",
        "\n",
        "        # Skip if word already replaced\n",
        "        if modern_word in replaced_words:\n",
        "            # print(f\"  â­ï¸ Skipping {modern_word} (already replaced)\")\n",
        "            continue\n",
        "\n",
        "        pattern = r'\\b' + re.escape(modern_word) + r'\\b'\n",
        "\n",
        "        # Only apply if word exists and hasn't been replaced yet\n",
        "        if re.search(pattern, processed_text) and f\"{{{modern_word}}}\" not in processed_text:\n",
        "            replacement = f\"{{{modern_word}}}[{traditional_word}]\"\n",
        "            processed_text = re.sub(pattern, replacement, processed_text)\n",
        "            replaced_words.add(modern_word)\n",
        "\n",
        "            replacement_log.append({\n",
        "                'word': modern_word,\n",
        "                'replacement': traditional_word,\n",
        "                'confidence': 0.7,  # Grok suggestions get medium confidence\n",
        "                'source': 'grok_filtered',\n",
        "                'priority': 'grok'\n",
        "            })\n",
        "            # print(f\"  âœ… Replaced: {modern_word} â†’ {traditional_word}\")\n",
        "        else:\n",
        "            print(f\"  â­ï¸ Skipping {modern_word} (not found or already replaced)\")\n",
        "\n",
        "    return processed_text, replacement_log\n",
        "\n",
        "\n",
        "\n",
        "class SmartModernWordFilter:\n",
        "    \"\"\"Filter modern Hebrew words to identify only those worth processing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Words that have NO traditional equivalent (modern concepts)\n",
        "        self.untranslatable_words = {\n",
        "            # Technology\n",
        "            \"××—×©×‘\", \"××™× ×˜×¨× ×˜\", \"×˜×œ×¤×•×Ÿ\", \"×¤×œ××¤×•×Ÿ\", \"××™×™×œ\", \"××™××™×™×œ\", \"×•×•××˜×¡××¤\",\n",
        "            \"×¤×™×™×¡×‘×•×§\", \"×’×•×’×œ\", \"××¤×œ×™×§×¦×™×”\", \"×ª×•×›× ×”\", \"×—×•××¨×”\", \"×¤×¨×•×’×¨××¨\",\n",
        "            \"×”××§×¨\", \"×•×™×¨×•×¡\", \"××—×©×•×‘\", \"×“×™×’×™×˜×œ×™\", \"××œ×§×˜×¨×•× ×™\", \"×¨×“×™×•\", \"×˜×œ×•×•×™×–×™×”\",\n",
        "            \"×•×™×“××•\", \"×¡×œ×¤×™\", \"×‘×œ×•×˜×•×ª'\", \"×•×•×™×¤×™\", \"GPS\", \"USB\", \"×¡×××¨×˜×¤×•×Ÿ\",\n",
        "\n",
        "            # Transportation\n",
        "            \"××›×•× ×™×ª\", \"××•×˜×•×‘×•×¡\", \"×¨×›×‘×ª\", \"××˜×•×¡\", \"××•×¤× ×•×¢\", \"××•×¤× ×™×™×\", \"××•× ×™×ª\",\n",
        "            \"×¨××–×•×¨\", \"×—× ×™×•×Ÿ\", \"×ª×—× ×ª ×“×œ×§\", \"×›×‘×™×© ××”×™×¨\", \"×’×©×¨ ×¢×œ×™×•×Ÿ\",\n",
        "\n",
        "            # Modern institutions/concepts\n",
        "            \"×‘× ×§\", \"×‘×™×˜×•×—\", \"××©×›× ×ª×\", \"×§×¨×“×™×˜\", \"×“×‘×™×˜\", \"×¢×¡×§×”\", \"×”×©×§×¢×”\",\n",
        "            \"×× ×™×”\", \"×‘×•×¨×¡×”\", \"××™× ×¤×œ×¦×™×”\", \"×“×¤×œ×¦×™×”\", \"GDP\", \"××‘×˜×œ×”\",\n",
        "            \"×“××•×§×¨×˜×™×”\", \"×§×¤×™×˜×œ×™×–×\", \"×¡×•×¦×™××œ×™×–×\", \"×§×•××•× ×™×–×\",\n",
        "\n",
        "            # Modern professions\n",
        "            \"×× ×”×œ\", \"××–×›×™×¨×”\", \"×¨×•××” ×—×©×‘×•×Ÿ\", \"×¢×•×¨×š ×“×™×Ÿ\", \"××”× ×“×¡\", \"××“×¨×™×›×œ\",\n",
        "            \"×¤×¡×™×›×•×œ×•×’\", \"×¡×•×¦×™×•×œ×•×’\", \"×¢×™×ª×•× ××™\", \"×¦×œ×\", \"×©×—×§×Ÿ\", \"×‘×××™\",\n",
        "\n",
        "            # Modern objects/items\n",
        "            \"××§×¨×¨\", \"×ª× ×•×¨\", \"××™×§×¨×•×’×œ\", \"××›×•× ×ª ×›×‘×™×¡×”\", \"×©×•××‘ ××‘×§\", \"××–×’×Ÿ\",\n",
        "            \"×©×¢×•×Ÿ ×™×“\", \"××©×§×¤×™×™×\", \"×¢×“×©×•×ª ××’×¢\", \"×ª×¨×•×¤×”\", \"×•×™×˜××™×Ÿ\",\n",
        "            \"×¤×œ×¡×˜×™×§\", \"× ×™×™×¨\", \"×¢×˜\", \"×¢×™×¤×¨×•×Ÿ\", \"××—×‘×¨×ª\", \"×¡×¤×¨ ×œ×™××•×“\",\n",
        "\n",
        "            # Sports and recreation\n",
        "            \"×›×“×•×¨×’×œ\", \"×›×“×•×¨×¡×œ\", \"×˜× ×™×¡\", \"×©×—×™×™×”\", \"×¨×™×¦×”\", \"×›×•×©×¨\", \"×—×“×¨ ×›×•×©×¨\",\n",
        "            \"××•×œ×™××¤×™××“×”\", \"××“×œ×™×”\", \"××œ×™×¤×•×ª\", \"×œ×™×’×”\", \"×§×‘×•×¦×”\",\n",
        "\n",
        "            # Modern food/products\n",
        "            \"×¤×™×¦×”\", \"×”××‘×•×¨×’×¨\", \"×¡× ×“×•×•×™×¥'\", \"×©×•×§×•×œ×“\", \"×’×œ×™×“×”\", \"×§×¤×”\", \"×ª×”\",\n",
        "            \"×§×•×§×” ×§×•×œ×\", \"××™× ××™× ×¨×œ×™×\", \"×™×•×’×•×¨×˜\", \"×§×•×¨× ×¤×œ×§×¡\",\n",
        "        }\n",
        "\n",
        "        # Words that are already traditional Hebrew - don't process these\n",
        "        self.already_traditional = {\n",
        "            # Religious terms\n",
        "            \"××œ×•×”×™×\", \"×”'\", \"×‘×•×¨×\", \"×§×“×•×©\", \"×‘×¨×•×š\", \"×××Ÿ\", \"×”×œ×œ×•×™×”\",\n",
        "            \"×ª×¤×™×œ×”\", \"×‘×¨×›×”\", \"××¦×•×•×”\", \"×ª×•×¨×”\", \"×ª× ×š\", \"××©× ×”\", \"×ª×œ××•×“\",\n",
        "            \"×©×‘×ª\", \"×—×’\", \"×¤×¡×—\", \"×¡×•×›×•×ª\", \"×©×‘×•×¢×•×ª\", \"×¨××© ×”×©× ×”\", \"×™×•× ×›×™×¤×•×¨\",\n",
        "            \"×‘×™×ª ×›× ×¡×ª\", \"×‘×™×ª ××“×¨×©\", \"×™×©×™×‘×”\", \"×›×”×Ÿ\", \"×œ×•×™\", \"×™×©×¨××œ\",\n",
        "\n",
        "            # Biblical Hebrew\n",
        "            \"× ×¤×©\", \"×¨×•×—\", \"×œ×‘\", \"×œ×‘×‘\", \"×¢×™×Ÿ\", \"××•×–×Ÿ\", \"×¤×”\", \"×œ×©×•×Ÿ\",\n",
        "            \"×™×“\", \"×¨×’×œ\", \"×¨××©\", \"×’×•×£\", \"×‘×©×¨\", \"×“×\", \"×¢×¦×\",\n",
        "            \"×©××™×\", \"××¨×¥\", \"×™×\", \"× ×”×¨\", \"×”×¨\", \"×’×‘×¢×”\", \"×¢××§\", \"××“×‘×¨\",\n",
        "            \"××•×¨\", \"×—×•×©×š\", \"×™×•×\", \"×œ×™×œ×”\", \"×‘×•×§×¨\", \"×¢×¨×‘\", \"×©× ×”\", \"×—×•×“×©\",\n",
        "\n",
        "            # Traditional values/concepts\n",
        "            \"×—×›××”\", \"×‘×™× ×”\", \"×“×¢×ª\", \"×××•× ×”\", \"×™×¨××”\", \"××”×‘×”\", \"×©××—×”\",\n",
        "            \"×©×œ×•×\", \"×¦×“×§\", \"×—×¡×“\", \"×¨×—××™×\", \"×¡×œ×™×—×”\", \"×ª×©×•×‘×”\",\n",
        "        }\n",
        "\n",
        "    def categorize_word(self, word: str) -> str:\n",
        "        \"\"\"Categorize a Hebrew word to determine if it should be processed\"\"\"\n",
        "        normalized_word = word.strip()\n",
        "\n",
        "        if normalized_word in self.already_traditional:\n",
        "            return ModernWordCategory.ALREADY_TRADITIONAL\n",
        "        elif normalized_word in self.untranslatable_words:\n",
        "            return ModernWordCategory.UNTRANSLATABLE\n",
        "        else:\n",
        "            # Default to contextual - let Grok decide but with lower priority\n",
        "            return ModernWordCategory.CONTEXTUAL\n",
        "\n",
        "    def filter_words_for_grok_processing(self, word_list: List[str]) -> List[str]:\n",
        "        \"\"\"Filter word list to include only words worth processing with Grok\"\"\"\n",
        "        filtered_words = []\n",
        "        skipped_count = 0\n",
        "\n",
        "        for word in word_list:\n",
        "            category = self.categorize_word(word)\n",
        "\n",
        "            # Only process contextual words (potentially translatable)\n",
        "            if category == ModernWordCategory.CONTEXTUAL:\n",
        "                filtered_words.append(word)\n",
        "            else:\n",
        "                skipped_count += 1\n",
        "                if skipped_count <= 5:  # Show first few skipped words\n",
        "                    print(f\"  Skipping '{word}' - {category}\")\n",
        "\n",
        "        if skipped_count > 5:\n",
        "            print(f\"  ... and {skipped_count - 5} more skipped words\")\n",
        "\n",
        "        return filtered_words\n",
        "\n",
        "class EnhancedHebrewTextProcessor:\n",
        "    def __init__(self, claude_api_key: str = None, grok_api_key: str = None):\n",
        "        self.claude_api_key = claude_api_key\n",
        "        self.grok_api_key = grok_api_key\n",
        "        self.csv_database = {}\n",
        "        self.context_patterns = defaultdict(list)\n",
        "        self.word_frequency = defaultdict(int)\n",
        "        self.processed_cache = {}\n",
        "\n",
        "        # NEW: Add smart filtering\n",
        "        self.word_filter = SmartModernWordFilter()\n",
        "\n",
        "        # Initialize SQLite for better data management\n",
        "        self.init_local_database()\n",
        "\n",
        "        # Hebrew morphology patterns\n",
        "        self.hebrew_prefixes = ['×‘', '×›', '×œ', '×', '×©', '×”', '×•']\n",
        "        self.hebrew_suffixes = ['×™×', '×•×ª', '×”', '×™', '×š', '× ×•', '×›×', '×”×', '×”×Ÿ']\n",
        "\n",
        "    def init_local_database(self):\n",
        "        \"\"\"Initialize local SQLite database for caching and analysis\"\"\"\n",
        "        self.conn = sqlite3.connect(':memory:')  # In-memory for Colab\n",
        "\n",
        "        self.conn.execute('''\n",
        "            CREATE TABLE words (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                modern_word TEXT UNIQUE,\n",
        "                traditional_word TEXT,\n",
        "                confidence REAL,\n",
        "                frequency INTEGER DEFAULT 1,\n",
        "                source TEXT,\n",
        "                context_patterns TEXT,\n",
        "                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        self.conn.execute('''\n",
        "            CREATE TABLE word_contexts (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                word_id INTEGER,\n",
        "                before_context TEXT,\n",
        "                after_context TEXT,\n",
        "                replacement_used TEXT,\n",
        "                confidence REAL,\n",
        "                FOREIGN KEY (word_id) REFERENCES words (id)\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        self.conn.commit()\n",
        "\n",
        "    def normalize_hebrew_text(self, text: str) -> str:\n",
        "        \"\"\"Advanced Hebrew text normalization\"\"\"\n",
        "        # Remove nikud (vowel points) for better matching\n",
        "        normalized = re.sub(r'[\\u0591-\\u05C7]', '', text)\n",
        "\n",
        "        # Normalize similar-looking Hebrew letters\n",
        "        letter_mappings = {\n",
        "            '×š': '×›', '×': '×', '×Ÿ': '× ', '×£': '×¤', '×¥': '×¦'  # Final forms\n",
        "        }\n",
        "\n",
        "        for final, regular in letter_mappings.items():\n",
        "            normalized = normalized.replace(final, regular)\n",
        "\n",
        "        return normalized.strip()\n",
        "\n",
        "    def extract_word_variants(self, word: str) -> List[str]:\n",
        "        \"\"\"Generate morphological variants of Hebrew words\"\"\"\n",
        "        variants = [word]\n",
        "        normalized = self.normalize_hebrew_text(word)\n",
        "\n",
        "        # Add variant with/without prefixes\n",
        "        for prefix in self.hebrew_prefixes:\n",
        "            if word.startswith(prefix) and len(word) > 2:\n",
        "                variants.append(word[1:])  # Without prefix\n",
        "            else:\n",
        "                variants.append(prefix + word)  # With prefix\n",
        "\n",
        "        # Add variant with/without common suffixes\n",
        "        for suffix in self.hebrew_suffixes:\n",
        "            if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
        "                variants.append(word[:-len(suffix)])  # Without suffix\n",
        "            else:\n",
        "                variants.append(word + suffix)  # With suffix\n",
        "\n",
        "        return list(set(variants))\n",
        "\n",
        "    def enhanced_google_sheet_loader(self, sheet_url: str, sheet_name: str = \"Sheet1\"):\n",
        "        \"\"\"Enhanced Google Sheets loader with validation and caching\"\"\"\n",
        "        import re\n",
        "\n",
        "        # Extract sheet ID\n",
        "        match = re.search(r'/spreadsheets/d/([a-zA-Z0-9-_]+)', sheet_url)\n",
        "        if not match:\n",
        "            raise ValueError(\"Invalid Google Sheets URL\")\n",
        "\n",
        "        sheet_id = match.group(1)\n",
        "        csv_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "\n",
        "        # Create cache key\n",
        "        cache_key = hashlib.md5(csv_url.encode()).hexdigest()\n",
        "\n",
        "        try:\n",
        "            print(\"Loading Google Sheets data...\")\n",
        "            df = pd.read_csv(csv_url, encoding='utf-8')\n",
        "\n",
        "            # Flexible column detection\n",
        "            if len(df.columns) >= 2:\n",
        "                # Auto-detect columns based on content\n",
        "                modern_col = df.columns[0]\n",
        "                traditional_col = df.columns[1]\n",
        "                source_col = df.columns[2] if len(df.columns) > 2 else None\n",
        "\n",
        "                print(f\"Detected columns: {modern_col} -> {traditional_col}\")\n",
        "\n",
        "                # Clean and validate data\n",
        "                df = df.dropna(subset=[modern_col, traditional_col])\n",
        "                df[modern_col] = df[modern_col].astype(str).str.strip()\n",
        "                df[traditional_col] = df[traditional_col].astype(str).str.strip()\n",
        "\n",
        "                # Filter out invalid entries\n",
        "                df = df[df[modern_col].str.len() > 0]\n",
        "                df = df[df[traditional_col].str.len() > 0]\n",
        "                df = df[df[modern_col] != df[traditional_col]]  # Exclude identical words\n",
        "\n",
        "                # Store in local database with metadata\n",
        "                for _, row in df.iterrows():\n",
        "                    modern_word = self.normalize_hebrew_text(row[modern_col])\n",
        "                    traditional_word = row[traditional_col]\n",
        "                    source = row[source_col] if source_col else 'google_sheet'\n",
        "\n",
        "                    self.conn.execute('''\n",
        "                        INSERT OR REPLACE INTO words\n",
        "                        (modern_word, traditional_word, confidence, source)\n",
        "                        VALUES (?, ?, ?, ?)\n",
        "                    ''', (modern_word, traditional_word, 0.8, source))\n",
        "\n",
        "                    # Also store variants\n",
        "                    for variant in self.extract_word_variants(modern_word):\n",
        "                        if variant != modern_word:\n",
        "                            self.conn.execute('''\n",
        "                                INSERT OR IGNORE INTO words\n",
        "                                (modern_word, traditional_word, confidence, source)\n",
        "                                VALUES (?, ?, ?, ?)\n",
        "                            ''', (variant, traditional_word, 0.6, f\"{source}_variant\"))\n",
        "\n",
        "                self.conn.commit()\n",
        "\n",
        "                # Update main dictionary for backward compatibility\n",
        "                self.csv_database = dict(zip(\n",
        "                    df[modern_col].apply(self.normalize_hebrew_text),\n",
        "                    df[traditional_col]\n",
        "                ))\n",
        "\n",
        "                print(f\"Successfully loaded {len(self.csv_database)} word pairs\")\n",
        "                print(f\"Generated {self.conn.execute('SELECT COUNT(*) FROM words').fetchone()[0]} total entries with variants\")\n",
        "\n",
        "                # Show sample\n",
        "                sample_items = list(self.csv_database.items())[:5]\n",
        "                for modern, traditional in sample_items:\n",
        "                    print(f\"  {modern} -> {traditional}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading Google Sheets: {e}\")\n",
        "\n",
        "    def context_aware_word_detection(self, text: str, window_size: int = 50) -> List[HebrewMatch]:\n",
        "        \"\"\"Enhanced word detection with context analysis\"\"\"\n",
        "        matches = []\n",
        "        normalized_text = self.normalize_hebrew_text(text)\n",
        "\n",
        "        # Get all words from database\n",
        "        cursor = self.conn.execute('''\n",
        "            SELECT modern_word, traditional_word, confidence, source\n",
        "            FROM words ORDER BY confidence DESC\n",
        "        ''')\n",
        "\n",
        "        for modern_word, traditional_word, confidence, source in cursor.fetchall():\n",
        "            # Find word boundaries more accurately\n",
        "            pattern = r'\\b' + re.escape(modern_word) + r'\\b'\n",
        "\n",
        "            for match in re.finditer(pattern, normalized_text):\n",
        "                start, end = match.span()\n",
        "\n",
        "                # Extract context\n",
        "                context_start = max(0, start - window_size)\n",
        "                context_end = min(len(normalized_text), end + window_size)\n",
        "\n",
        "                context_before = normalized_text[context_start:start].strip()\n",
        "                context_after = normalized_text[end:context_end].strip()\n",
        "\n",
        "                # Calculate context-based confidence adjustment\n",
        "                context_confidence = self.calculate_context_confidence(\n",
        "                    modern_word, context_before, context_after\n",
        "                )\n",
        "\n",
        "                adjusted_confidence = min(1.0, confidence * context_confidence)\n",
        "\n",
        "                matches.append(HebrewMatch(\n",
        "                    word=modern_word,\n",
        "                    traditional_equivalent=traditional_word,\n",
        "                    confidence=adjusted_confidence,\n",
        "                    context_before=context_before,\n",
        "                    context_after=context_after,\n",
        "                    position=(start, end),\n",
        "                    source=source\n",
        "                ))\n",
        "\n",
        "        # Sort by confidence and remove duplicates\n",
        "        matches.sort(key=lambda x: x.confidence, reverse=True)\n",
        "        return self.remove_overlapping_matches(matches)\n",
        "\n",
        "    def calculate_context_confidence(self, word: str, before: str, after: str) -> float:\n",
        "        \"\"\"Calculate confidence based on surrounding context\"\"\"\n",
        "        confidence = 1.0\n",
        "\n",
        "        # Religious/spiritual context indicators (boost confidence)\n",
        "        religious_indicators = ['× ×¤×©', '×¨×•×—', '××œ×•×§×™', '×§×“×•×©', '×‘×¨×•×š', '×ª×¤×™×œ×”', '××¦×•×•×”']\n",
        "        modern_indicators = ['×˜×›× ×•×œ×•×’×™×”', '××™× ×˜×¨× ×˜', '××—×©×‘', '×¤×œ××¤×•×Ÿ']\n",
        "\n",
        "        context_text = before + ' ' + after\n",
        "\n",
        "        # Boost confidence for religious context\n",
        "        for indicator in religious_indicators:\n",
        "            if indicator in context_text:\n",
        "                confidence *= 1.2\n",
        "\n",
        "        # Reduce confidence for modern context\n",
        "        for indicator in modern_indicators:\n",
        "            if indicator in context_text:\n",
        "                confidence *= 0.8\n",
        "\n",
        "        return min(1.0, confidence)\n",
        "\n",
        "    def remove_overlapping_matches(self, matches: List[HebrewMatch]) -> List[HebrewMatch]:\n",
        "        \"\"\"Remove overlapping matches, keeping higher confidence ones\"\"\"\n",
        "        if not matches:\n",
        "            return []\n",
        "\n",
        "        # Sort by position\n",
        "        matches.sort(key=lambda x: x.position[0])\n",
        "\n",
        "        filtered = [matches[0]]\n",
        "\n",
        "        for current in matches[1:]:\n",
        "            last_match = filtered[-1]\n",
        "\n",
        "            # Check for overlap\n",
        "            if current.position[0] < last_match.position[1]:\n",
        "                # Overlapping - keep higher confidence\n",
        "                if current.confidence > last_match.confidence:\n",
        "                    filtered[-1] = current\n",
        "            else:\n",
        "                filtered.append(current)\n",
        "\n",
        "        return filtered\n",
        "\n",
        "    def chunk_words_for_grok(self, words: List[str], max_words_per_chunk: int = 250) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        FIXED: Chunk filtered words (not sentences) for Grok processing\n",
        "        \"\"\"\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "\n",
        "        for word in words:\n",
        "            current_chunk.append(word)\n",
        "\n",
        "            if len(current_chunk) >= max_words_per_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "                current_chunk = []\n",
        "\n",
        "        # Add remaining words if any\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk)\n",
        "\n",
        "        print(f\"Created {len(chunks)} word chunks from {len(words)} filtered words\")\n",
        "        return chunks\n",
        "\n",
        "\n",
        "    def validate_suggestions_with_grok(self, raw_suggestions: Dict[str, str]) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Validate Grok's initial suggestions using a second Grok API call\n",
        "\n",
        "        Args:\n",
        "            raw_suggestions: Dictionary of modern_word: traditional_equivalent from first pass\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of validated suggestions (only the ones that pass validation)\n",
        "        \"\"\"\n",
        "        if not self.grok_api_key or not raw_suggestions:\n",
        "            return {}\n",
        "\n",
        "        print(f\"\\n=== GROK VALIDATION PASS ===\")\n",
        "        print(f\"Validating {len(raw_suggestions)} initial suggestions...\")\n",
        "\n",
        "        # Create validation prompt\n",
        "        validation_prompt = f\"\"\"\n",
        "            You are a Hebrew linguistics expert reviewing translation suggestions for errors.\n",
        "\n",
        "            TASK: Validate these modernâ†’traditional Hebrew translations. Return ONLY the ones that are correct.\n",
        "\n",
        "            TRANSLATIONS TO VALIDATE:\n",
        "            {json.dumps(raw_suggestions, ensure_ascii=False, indent=2)}\n",
        "\n",
        "            VALIDATION CRITERIA:\n",
        "            âŒ REJECT if:\n",
        "            - Wrong meaning entirely (semantic mismatch)\n",
        "            - Over-elevation (mundaneâ†’sacred inappropriately)\n",
        "            - Under-elevation (sacredâ†’mundane inappropriately)\n",
        "            - Modern word disguised as traditional\n",
        "            - Not actually from traditional Hebrew sources (biblical/mishnaic/medieval)\n",
        "            - False cognates or sound-alike words with different meanings\n",
        "\n",
        "            âœ… KEEP if:\n",
        "            - Exact semantic match between modern and traditional word\n",
        "            - Appropriate intensity level for the context\n",
        "            - Genuinely traditional Hebrew from authentic sources\n",
        "            - Natural usage in traditional Hebrew contexts\n",
        "            - Preserves the original meaning precisely\n",
        "\n",
        "            CRITICAL EXAMPLES OF ERRORS TO CATCH:\n",
        "            âŒ \"××©×™××”\" â†’ \"×©×œ×™×—×•×ª\" (task â‰  divine mission - wrong intensity level)\n",
        "            âŒ \"×¤×’×™×¢×•×ª\" â†’ \"×—×•×œ×™×Ÿ\" (vulnerability â‰  secular/mundane - completely wrong meaning)\n",
        "            âŒ \"×‘×™× ×•× ×™×•×ª\" â†’ \"×¤×©×˜×•×ª\" (mediocrity â‰  simplicity - different concepts entirely)\n",
        "            âŒ \"×”×©×¨××”\" â†’ \"× ×‘×•××”\" (inspiration â‰  prophecy - too specific/elevated)\n",
        "            âŒ \"×¢×•×•×œ\" â†’ \"×¢×•×œ\" (injustice â‰  yoke - false cognate)\n",
        "\n",
        "            EXAMPLES OF GOOD TRANSLATIONS TO KEEP:\n",
        "            âœ… \"×¤×—×“×™×\" â†’ \"×™×¨××”\" (fears â†’ awe - appropriate spiritual elevation)\n",
        "            âœ… \"××ª×’×¨\" â†’ \"× ×¡×™×•×Ÿ\" (challenge â†’ test/trial - perfect semantic match)\n",
        "            âœ… \"×¢×¦×‘×•×ª\" â†’ \"×“××‘×”\" (sadness â†’ sorrow - authentic biblical term)\n",
        "\n",
        "            CRITICAL INSTRUCTION:\n",
        "            When in doubt, REJECT the suggestion.\n",
        "            Only approve translations where you are confident of:\n",
        "            1. Semantic accuracy\n",
        "            2. Appropriate register/intensity\n",
        "            3. Traditional Hebrew authenticity\n",
        "\n",
        "            RESPONSE FORMAT:\n",
        "            Return ONLY the validated translations as clean JSON:\n",
        "            {{\"modern_word\": \"traditional_equivalent\"}}\n",
        "\n",
        "            If a translation fails validation, omit it entirely from the response.\n",
        "            Do not include explanations, just the validated JSON object.\n",
        "            \"\"\"\n",
        "\n",
        "        try:\n",
        "            headers = {\n",
        "                'Content-Type': 'application/json',\n",
        "                'Authorization': f'Bearer {self.grok_api_key}'\n",
        "            }\n",
        "\n",
        "            data = {\n",
        "                'model': 'grok-3-latest',\n",
        "                'messages': [{'role': 'user', 'content': validation_prompt}],\n",
        "                'max_tokens': 3500,\n",
        "                'temperature': 0.1  # Very low temperature for consistent validation\n",
        "            }\n",
        "\n",
        "            print(\"Sending validation request to Grok...\")\n",
        "\n",
        "            response = requests.post(\n",
        "                'https://api.x.ai/v1/chat/completions',\n",
        "                headers=headers,\n",
        "                json=data,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                content = result['choices'][0]['message']['content']\n",
        "\n",
        "                print(\"Received validation response, parsing...\")\n",
        "\n",
        "                # Extract JSON from response\n",
        "                json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
        "                json_matches = re.findall(json_pattern, content, re.DOTALL)\n",
        "\n",
        "                validated_suggestions = {}\n",
        "\n",
        "                for json_match in json_matches:\n",
        "                    try:\n",
        "                        parsed_suggestions = json.loads(json_match)\n",
        "                        if isinstance(parsed_suggestions, dict):\n",
        "                            # Additional safety check - ensure suggested words were in original\n",
        "                            for modern, traditional in parsed_suggestions.items():\n",
        "                                if modern in raw_suggestions:\n",
        "                                    validated_suggestions[modern] = traditional\n",
        "                                else:\n",
        "                                    print(f\"  âš ï¸  Validation added unknown word: {modern} (ignored)\")\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"  âŒ JSON parsing error in validation: {e}\")\n",
        "                        continue\n",
        "\n",
        "                # Calculate and display validation statistics\n",
        "                original_count = len(raw_suggestions)\n",
        "                validated_count = len(validated_suggestions)\n",
        "                rejection_rate = ((original_count - validated_count) / original_count * 100) if original_count > 0 else 0\n",
        "\n",
        "                print(f\"\\nğŸ“Š VALIDATION RESULTS:\")\n",
        "                print(f\"Original suggestions: {original_count}\")\n",
        "                print(f\"Validated suggestions: {validated_count}\")\n",
        "                print(f\"Rejected suggestions: {original_count - validated_count}\")\n",
        "                print(f\"Rejection rate: {rejection_rate:.1f}%\")\n",
        "\n",
        "                # Show what was rejected (for debugging)\n",
        "                # rejected = set(raw_suggestions.keys()) - set(validated_suggestions.keys())\n",
        "                # if rejected:\n",
        "                #     print(f\"\\nğŸ—‘ï¸  REJECTED SUGGESTIONS:\")\n",
        "                #     for word in rejected:\n",
        "                #         print(f\"   {word} â†’ {raw_suggestions[word]} (filtered out)\")\n",
        "\n",
        "                # print(f\"\\nâœ… VALIDATED SUGGESTIONS:\")\n",
        "                # for modern, traditional in validated_suggestions.items():\n",
        "                #     print(f\"   {modern} â†’ {traditional}\")\n",
        "\n",
        "                return validated_suggestions\n",
        "\n",
        "            else:\n",
        "                print(f\"âŒ Grok validation API error: {response.status_code}\")\n",
        "                if response.status_code == 429:\n",
        "                    print(\"   Rate limit hit - consider adding delays between calls\")\n",
        "                print(f\"   Response: {response.text}\")\n",
        "\n",
        "                # Fallback: return original suggestions with warning\n",
        "                print(\"âš ï¸  Falling back to unvalidated suggestions\")\n",
        "                return raw_suggestions\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error in Grok validation call: {e}\")\n",
        "            # Fallback: return original suggestions\n",
        "            print(\"âš ï¸  Falling back to unvalidated suggestions\")\n",
        "            return raw_suggestions\n",
        "\n",
        "\n",
        "    def enhanced_grok_api_call(self, text: str, known_words: List[str]) -> Dict[str, str]:\n",
        "        \"\"\"ENHANCED: Grok API call with smart filtering integration\"\"\"\n",
        "        if not self.grok_api_key:\n",
        "            return {}\n",
        "\n",
        "        print(f\"\\n=== SMART FILTERING FOR GROK ===\")\n",
        "        print(f\"Words detected from database matching: {len(known_words)}\")\n",
        "\n",
        "        # NEW: Apply smart filtering before processing\n",
        "        # Extract words that appear in the text for filtering\n",
        "        text_words = re.findall(r'[\\u0590-\\u05FF]+', text)  # Extract Hebrew words\n",
        "        unique_text_words = list(set(text_words))\n",
        "\n",
        "        # Filter words to only those worth sending to Grok\n",
        "        filtered_words = self.word_filter.filter_words_for_grok_processing(unique_text_words)\n",
        "\n",
        "        print(f\"Words in text: {len(unique_text_words)}\")\n",
        "        print(f\"Words to process with Grok: {len(filtered_words)}\")\n",
        "        # print(f\"API calls saved by filtering: {len(unique_text_words) - len(filtered_words)}\")\n",
        "\n",
        "        if not filtered_words:\n",
        "            print(\"No words need Grok processing - all are either untranslatable or traditional!\")\n",
        "            return {}\n",
        "\n",
        "        # Create cache key for this text segment\n",
        "        text_hash = hashlib.md5(text.encode()).hexdigest()\n",
        "\n",
        "        # Check cache first\n",
        "        if text_hash in self.processed_cache:\n",
        "            return self.processed_cache[text_hash]\n",
        "\n",
        "        sentences = self.split_into_sentences(text)\n",
        "        sentence_chunks = self.chunk_sentences(sentences, max_tokens=2500)\n",
        "\n",
        "        all_suggestions = {}\n",
        "\n",
        "        for i, sentence_chunk in enumerate(tqdm(sentence_chunks, desc=\"Processing with Grok\")):\n",
        "            chunk_text = \" \".join(sentence_chunk)\n",
        "\n",
        "            # ENHANCED: Better prompt with filtering guidance\n",
        "            sample_untranslatable = list(self.word_filter.untranslatable_words)[:15]\n",
        "            sample_traditional = list(self.word_filter.already_traditional)[:10]\n",
        "\n",
        "            # Enhanced prompt with examples and constraints\n",
        "            prompt = f\"\"\"\n",
        "            You are a scholar of Lashon Hakodesh (Sacred Hebrew) specializing in religious and spiritual texts.\n",
        "\n",
        "            TASK: Transform modern Hebrew words into their traditional/biblical equivalents while preserving exact meaning and spiritual depth.\n",
        "\n",
        "            CRITICAL CONTEXT: This text discusses spiritual matters - souls, divine service, and religious psychology. Choose traditional terms that ENHANCE rather than diminish sacred concepts\n",
        "\n",
        "            TEXT TO ANALYZE:\n",
        "            {chunk_text}\n",
        "\n",
        "\n",
        "            PRECISION RULES:\n",
        "            1. Verify semantic equivalence - meaning must match exactly\n",
        "            2. Preserve spiritual intensity - prefer elevated over mundane language\n",
        "            3  Secular terminology with religious equivalents\n",
        "            4. Use only authentic traditional Hebrew from Tanakh/Mishna/medieval sources\n",
        "            5. If modern word is already traditional, suggest keeping it\n",
        "\n",
        "\n",
        "            CRITICAL EXCLUSIONS - DO NOT suggest traditional equivalents for:\n",
        "            âŒ Modern technology: {sample_untranslatable}\n",
        "            âŒ Modern objects that didn't exist in ancient times\n",
        "            âŒ Already traditional terms: {sample_traditional}\n",
        "            âŒ Words already in our database: {known_words[:50]}\n",
        "\n",
        "\n",
        "            EXAMPLES of excellence:\n",
        "            âœ“ \"×¤×—×“×™×\" â†’ \"×™×¨××”\" (fears â†’ sacred awe - deeper spiritual meaning)\n",
        "            âœ“ \"××ª×’×¨\" â†’ \"× ×¡×™×•×Ÿ\" (challenge â†’ divine test - theological precision)\n",
        "            âœ“ \"×©×¢××•×\" â†’ \"×©×™×××•×Ÿ\" (boredom â†’ desolation - creative biblical root usage)\n",
        "\n",
        "            COMMON ERRORS TO AVOID:\n",
        "            âŒ Semantic mismatch: \"×¤×’×™×¢×•×ª\" â‰  \"×—×•×œ×™×Ÿ\" (vulnerability â‰  secular)\n",
        "            âŒ Over-elevation: \"××©×™××”\" â‰  \"×©×œ×™×—×•×ª\" (task â‰  divine mission)\n",
        "            âŒ Wrong specificity: \"×”×©×¨××”\" â‰  \"× ×‘×•××”\" (inspiration â‰  prophecy)\n",
        "            âŒ False cognates: \"×¢×•×•×œ\" â‰  \"×¢×•×œ\" (injustice â‰  yoke)\n",
        "            âŒ Modern slang as traditional: \"×ª×¡×›×•×œ\", \"×¤×¡×¤×•×¡\" are NOT traditional\n",
        "\n",
        "\n",
        "            RESPONSE FORMAT:\n",
        "            Return ONLY valid JSON: {{\"modern_word\": \"traditional_equivalent\"}}\n",
        "            If no suitable words found, return: {{}}\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                headers = {\n",
        "                    'Content-Type': 'application/json',\n",
        "                    'Authorization': f'Bearer {self.grok_api_key}'\n",
        "                }\n",
        "\n",
        "                data = {\n",
        "                    'model': 'grok-3-latest',\n",
        "                    'messages': [{'role': 'user', 'content': prompt}],\n",
        "                    'max_tokens': 4000,\n",
        "                    'temperature': 0.2  # Lower temperature for consistency\n",
        "                }\n",
        "\n",
        "                response = requests.post(\n",
        "                    'https://api.x.ai/v1/chat/completions',\n",
        "                    headers=headers,\n",
        "                    json=data,\n",
        "                    timeout=30\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    content = result['choices'][0]['message']['content']\n",
        "\n",
        "                    # Better JSON extraction\n",
        "                    json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
        "                    json_matches = re.findall(json_pattern, content, re.DOTALL)\n",
        "\n",
        "                    for json_match in json_matches:\n",
        "                        try:\n",
        "                            chunk_suggestions = json.loads(json_match)\n",
        "                            if isinstance(chunk_suggestions, dict):\n",
        "                                # Filter and validate suggestions with our smart filter\n",
        "                                filtered = self.validate_grok_suggestions_with_filter(chunk_suggestions, known_words)\n",
        "                                # filtered = self.validate_grok_suggestions_strict(chunk_suggestions,known_words,filtered_words)\n",
        "                                all_suggestions.update(filtered)\n",
        "                        except json.JSONDecodeError:\n",
        "                            continue\n",
        "\n",
        "                else:\n",
        "                    print(f\"Grok API error: {response.status_code}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in Grok API call: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Cache results\n",
        "        self.processed_cache[text_hash] = all_suggestions\n",
        "        return all_suggestions\n",
        "\n",
        "\n",
        "    def validate_grok_suggestions_strict(self, suggestions: Dict[str, str],\n",
        "                                   known_words: List[str],\n",
        "                                   original_word_chunk: List[str]) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        FIXED: Strict validation that ensures Grok only suggests words from our filtered list\n",
        "        \"\"\"\n",
        "        validated = {}\n",
        "\n",
        "        for modern, traditional in suggestions.items():\n",
        "            # 1. Skip if already in our database\n",
        "            if modern in known_words or modern in self.csv_database:\n",
        "                print(f\"    â­ï¸ Skipping {modern} (already in database)\")\n",
        "                continue\n",
        "\n",
        "            # 2. CRITICAL: Only accept words that were in our filtered chunk\n",
        "            if modern not in original_word_chunk:\n",
        "                print(f\"    âŒ Rejecting {modern} (not in filtered words)\")\n",
        "                continue\n",
        "\n",
        "            # 3. Check with our smart filter again\n",
        "            category = self.word_filter.categorize_word(modern)\n",
        "            if category in [ModernWordCategory.UNTRANSLATABLE, ModernWordCategory.ALREADY_TRADITIONAL]:\n",
        "                print(f\"    âŒ Rejecting {modern} ({category})\")\n",
        "                continue\n",
        "\n",
        "            # 4. Basic validation\n",
        "            if (len(modern.strip()) > 1 and\n",
        "                len(traditional.strip()) > 1 and\n",
        "                modern != traditional and\n",
        "                self.is_hebrew_word(modern) and\n",
        "                self.is_hebrew_word(traditional)):\n",
        "\n",
        "                validated[modern] = traditional\n",
        "                print(f\"    âœ… Validated: {modern} â†’ {traditional}\")\n",
        "            else:\n",
        "                print(f\"    âŒ Failed basic validation: {modern} â†’ {traditional}\")\n",
        "\n",
        "            return validated\n",
        "\n",
        "    def validate_grok_suggestions_with_filter(self, suggestions: Dict[str, str], known_words: List[str]) -> Dict[str, str]:\n",
        "        \"\"\"ENHANCED: Validate Grok suggestions with smart filtering\"\"\"\n",
        "        validated = {}\n",
        "\n",
        "        for modern, traditional in suggestions.items():\n",
        "            # Skip if already in database\n",
        "            if modern in known_words or modern in self.csv_database:\n",
        "                continue\n",
        "\n",
        "            # NEW: Check with our smart filter\n",
        "            category = self.word_filter.categorize_word(modern)\n",
        "            if category in [ModernWordCategory.UNTRANSLATABLE, ModernWordCategory.ALREADY_TRADITIONAL]:\n",
        "                print(f\"  Filtered out Grok suggestion: {modern} ({category})\")\n",
        "                continue\n",
        "\n",
        "            # Basic validation\n",
        "            if (len(modern.strip()) > 1 and\n",
        "                len(traditional.strip()) > 1 and\n",
        "                modern != traditional and\n",
        "                self.is_hebrew_word(modern) and\n",
        "                self.is_hebrew_word(traditional)):\n",
        "\n",
        "                validated[modern] = traditional\n",
        "\n",
        "        return validated\n",
        "\n",
        "\n",
        "    def grok_call_with_valiadations(self, text: str, known_words: list) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Enhanced version of your existing Grok call with validation\n",
        "        \"\"\"\n",
        "        # Step 1: Generate initial suggestions\n",
        "        raw_suggestions = self.enhanced_grok_api_call(text, known_words)\n",
        "\n",
        "        if not raw_suggestions:\n",
        "            return {}\n",
        "\n",
        "        # Step 2: Validate suggestions\n",
        "        validated_suggestions = self.validate_suggestions_with_grok(raw_suggestions)\n",
        "\n",
        "        return validated_suggestions\n",
        "\n",
        "\n",
        "\n",
        "    def is_hebrew_word(self, word: str) -> bool:\n",
        "        \"\"\"Check if word contains Hebrew characters\"\"\"\n",
        "        hebrew_pattern = r'[\\u0590-\\u05FF]'\n",
        "        return bool(re.search(hebrew_pattern, word))\n",
        "\n",
        "    def split_into_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Improved Hebrew sentence splitting\"\"\"\n",
        "        # Hebrew punctuation marks\n",
        "        sentence_endings = r'[.!?×ƒÖ‰]'\n",
        "\n",
        "        # Split while preserving punctuation\n",
        "        parts = re.split(f'({sentence_endings})', text)\n",
        "\n",
        "        sentences = []\n",
        "        i = 0\n",
        "        while i < len(parts):\n",
        "            sentence = parts[i].strip()\n",
        "            if i + 1 < len(parts) and re.match(sentence_endings, parts[i + 1]):\n",
        "                sentence += parts[i + 1]\n",
        "                i += 2\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "            if sentence and len(sentence) > 5:  # Filter very short segments\n",
        "                sentences.append(sentence)\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    def chunk_sentences(self, sentences: List[str], max_tokens: int = 2500) -> List[List[str]]:\n",
        "        \"\"\"Smart sentence chunking with context preservation\"\"\"\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Estimate tokens (Hebrew chars / 1.5 + spaces)\n",
        "            estimated_tokens = len(sentence.replace(' ', '')) // 1.5 + sentence.count(' ')\n",
        "\n",
        "            if current_length + estimated_tokens > max_tokens and current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "                current_chunk = [sentence]\n",
        "                current_length = estimated_tokens\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_length += estimated_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def generate_analysis_report(self, matches: List[HebrewMatch], grok_suggestions: Dict[str, str] = None) -> str:\n",
        "        \"\"\"Enhanced analysis report with filtering statistics\"\"\"\n",
        "        report = [\"=== HEBREW TEXT ANALYSIS REPORT WITH SMART FILTERING ===\\n\"]\n",
        "\n",
        "        # Summary statistics\n",
        "        total_matches = len(matches)\n",
        "        high_confidence = len([m for m in matches if m.confidence > 0.8])\n",
        "        medium_confidence = len([m for m in matches if 0.5 < m.confidence <= 0.8])\n",
        "        low_confidence = len([m for m in matches if m.confidence <= 0.5])\n",
        "\n",
        "        report.append(f\"Total modern words found: {total_matches}\")\n",
        "        report.append(f\"High confidence (>0.8): {high_confidence}\")\n",
        "        report.append(f\"Medium confidence (0.5-0.8): {medium_confidence}\")\n",
        "        report.append(f\"Low confidence (â‰¤0.5): {low_confidence}\")\n",
        "\n",
        "        if grok_suggestions:\n",
        "            report.append(f\"Additional Grok suggestions: {len(grok_suggestions)}\")\n",
        "\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Group by source\n",
        "        by_source = defaultdict(list)\n",
        "        for match in matches:\n",
        "            by_source[match.source].append(match)\n",
        "\n",
        "        for source, source_matches in by_source.items():\n",
        "            report.append(f\"\\n--- {(source or 'UNKNOWN').upper()} SUGGESTIONS ---\")\n",
        "            for match in sorted(source_matches, key=lambda x: x.confidence, reverse=True)[:10]:\n",
        "                report.append(f\"{match.word} â†’ {match.traditional_equivalent} (confidence: {match.confidence:.2f})\")\n",
        "\n",
        "        if grok_suggestions:\n",
        "            report.append(f\"\\n--- GROK API SUGGESTIONS (FILTERED) ---\")\n",
        "            for modern, traditional in list(grok_suggestions.items())[:30]:\n",
        "                report.append(f\"{modern} â†’ {traditional}\")\n",
        "\n",
        "        return \"\\n\".join(report)\n",
        "\n",
        "# Enhanced main function with smart filtering\n",
        "def enhanced_main():\n",
        "    \"\"\"Enhanced main function with smart filtering integration\"\"\"\n",
        "\n",
        "    # You'll need to define these API keys\n",
        "    # antropic_api_key = antropic_api_key  # Add your key here\n",
        "    # grok_api_key = grok_api_key          # Add your key here\n",
        "\n",
        "    # Initialize enhanced processor with filtering\n",
        "    processor = EnhancedHebrewTextProcessor(\n",
        "        claude_api_key=antropic_api_key,\n",
        "        grok_api_key=grok_api_key\n",
        "    )\n",
        "\n",
        "    # Load database with enhancements\n",
        "    sheet_url = \"https://docs.google.com/spreadsheets/d/14iTTuymFbo-pcepnd1k6djBiSgkL6TSVK6b3_DVWzgQ/edit?usp=sharing\"\n",
        "    sheet_url2 = \"https://docs.google.com/spreadsheets/d/1eXWRIA8LTk7fvQdn_-b1VT9Ao-V3CGMg1a0BdsJbZ34/edit?usp=sharing\"\n",
        "    processor.enhanced_google_sheet_loader(sheet_url2)\n",
        "\n",
        "    # Get document text\n",
        "    print(\"Please upload your Hebrew document...\")\n",
        "    text = scan_uploaded_docx_files()\n",
        "\n",
        "    if not text:\n",
        "        print(\"No text to process\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing document with {len(text)} characters...\")\n",
        "\n",
        "    # Enhanced word detection with context analysis\n",
        "    matches = processor.context_aware_word_detection(text)\n",
        "    print(f\"Found {len(matches)} potential replacements from database\")\n",
        "\n",
        "\n",
        "\n",
        "    # Get additional suggestions from Grok (with smart filtering)\n",
        "    known_words = list(processor.csv_database.keys())\n",
        "    # grok_suggestions = processor.enhanced_grok_api_call(text, known_words)\n",
        "    grok_suggestions = processor.grok_call_with_valiadations(text, known_words)\n",
        "    print(f\"Grok provided {len(grok_suggestions)} additional suggestions\")\n",
        "\n",
        "\n",
        "    # print(grok_suggestions)\n",
        "    # with open('datagrok.json', 'w', encoding='utf-8') as f:\n",
        "    #           json.dump(grok_suggestions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Load later\n",
        "    # with open('datagrok.json', 'r', encoding='utf-8') as f:\n",
        "    #     grok_suggestions = json.load(f)\n",
        "\n",
        "    import re\n",
        "\n",
        "    # Combine all suggestions\n",
        "    all_replacements = {**processor.csv_database, **grok_suggestions}\n",
        "\n",
        "    # Apply replacements with confidence scoring\n",
        "    processed_text = text\n",
        "    replacement_log = []\n",
        "\n",
        "\n",
        "     # Apply replacements with the fixed logic\n",
        "    processed_text, replacement_log = apply_replacements_safely(text, matches, grok_suggestions)\n",
        "\n",
        "    # Generate enhanced analysis report\n",
        "    report = processor.generate_analysis_report(matches, grok_suggestions)\n",
        "\n",
        "    # Create filtering statistics\n",
        "    import re\n",
        "\n",
        "    # 1) Compile your Hebrewâ€letter regex once\n",
        "    heb_re = re.compile(r'[\\u0590-\\u05FF]+')\n",
        "\n",
        "    # 2) Extract words and compute basics\n",
        "    all_words       = heb_re.findall(text)\n",
        "    unique_words    = set(all_words)\n",
        "    db_hits         = len(matches)\n",
        "    to_grok         = processor.word_filter.filter_words_for_grok_processing(list(unique_words))\n",
        "    grok_hits       = len(grok_suggestions)\n",
        "\n",
        "    # 3) Tally replacements\n",
        "    total_repl      = len(replacement_log)\n",
        "    db_repl_count   = sum(1 for r in replacement_log if r['source'] != 'grok_filtered')\n",
        "    grok_repl_count = sum(1 for r in replacement_log if r['source'] == 'grok_filtered')\n",
        "\n",
        "    # 4) Build the report with a plain f-string\n",
        "    filtering_stats = f\"\"\"\n",
        "    === SMART FILTERING STATISTICS ===\n",
        "    Total words in text: {len(all_words)}\n",
        "    Unique words in text: {len(unique_words)}\n",
        "    Words from database: {db_hits}\n",
        "    Words sent to Grok: {len(to_grok)}\n",
        "    Grok suggestions received: {grok_hits}\n",
        "    Total replacements applied: {total_repl}\n",
        "\n",
        "    === REPLACEMENT BREAKDOWN ===\n",
        "    Database replacements: {db_repl_count}\n",
        "    Grok replacements: {grok_repl_count}\n",
        "    \"\"\"\n",
        "\n",
        "    # Save outputs\n",
        "    with open('enhanced_processed.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(processed_text)\n",
        "\n",
        "    with open('analysis_report.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(report + \"\\n\\n\" + filtering_stats)\n",
        "\n",
        "    with open('replacement_log.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(replacement_log, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open('grok_suggestions.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(grok_suggestions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Download files\n",
        "    files.download('enhanced_processed.txt')\n",
        "    files.download('analysis_report.txt')\n",
        "    # files.download('replacement_log.json')\n",
        "    # files.download('grok_suggestions.json')\n",
        "\n",
        "    print(f\"\\nğŸ‰ PROCESSING COMPLETE! ğŸ‰\")\n",
        "    print(f\"ğŸ“Š Applied {len(replacement_log)} total replacements\")\n",
        "    print(f\"ğŸ—„ï¸ Database replacements: {len([r for r in replacement_log if r['source'] != 'grok_filtered'])}\")\n",
        "    print(f\"ğŸ¤– Grok replacements: {len([r for r in replacement_log if r['source'] == 'grok_filtered'])}\")\n",
        "    # print(f\"ğŸ’° API calls saved by filtering: {len(set(re.findall(r'[\\u0590-\\u05FF]+', text))) - len(processor.word_filter.filter_words_for_grok_processing(list(set(re.findall(r'[\\u0590-\\u05FF]+', text)))))}\")\n",
        "    print(f\"ğŸ“ Downloaded files:\")\n",
        "    print(f\"   - enhanced_processed.txt (main output)\")\n",
        "    print(f\"   - analysis_report.txt (detailed analysis)\")\n",
        "    print(f\"   - replacement_log.json (all replacements)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the demo first to see how filtering works\n",
        "    # demo_filtering()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Ready to run enhanced_main()!\")\n",
        "    print(\"Make sure to:\")\n",
        "    print(\"1. Set your API keys in the enhanced_main() function\")\n",
        "    enhanced_main()\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xfYpVT_QcoUf",
        "outputId": "dd1b6a6f-15ba-41f7-c861-c5b1acad14ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Ready to run enhanced_main()!\n",
            "Make sure to:\n",
            "1. Set your API keys in the enhanced_main() function\n",
            "Loading Google Sheets data...\n",
            "Detected columns: word -> traditional\n",
            "Successfully loaded 220 word pairs\n",
            "Generated 3731 total entries with variants\n",
            "  ××œ×™×œ -> ××¤×¡ ×—×¡×¨ ×›×•×—\n",
            "  ××—×– -> ×—×œ×§ ××—×“ ××××”, ×××™×ª\n",
            "  ×× ×™ ××××™×  -> ×©×œ×•×©×”â€‘×¢×©×¨ ×¢×™×§×¨×™ ×××•× ×”\n",
            "  ×× ×› -> ×§×• × ×™×¦×‘/×™×•×¨×“ ××Ÿ ×”×’×•×‘×”\n",
            "  ××™×© ××¢×©×” -> ×‘×¢×œ ××¦×•×•×ª; ×—×¡×™×“\n",
            "Please upload your Hebrew document...\n",
            "Please select HEBREW DOCX files to upload...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-67c96ca1-ee09-4089-8b62-2c0ff4916487\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-67c96ca1-ee09-4089-8b62-2c0ff4916487\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ×”×¤×—×“ ××—×•×¡×¨ ×¢×¨×š ×•××©×™××ª ×”×—×™×™× ×©×œ ××™×© ×”×¨×•1 ×§×œ×•×“ (1) (2).docx to ×”×¤×—×“ ××—×•×¡×¨ ×¢×¨×š ×•××©×™××ª ×”×—×™×™× ×©×œ ××™×© ×”×¨×•1 ×§×œ×•×“ (1) (2) (4).docx\n",
            "\n",
            "Scanning file: ×”×¤×—×“ ××—×•×¡×¨ ×¢×¨×š ×•××©×™××ª ×”×—×™×™× ×©×œ ××™×© ×”×¨×•1 ×§×œ×•×“ (1) (2) (4).docx\n",
            "Processing document with 8982 characters...\n",
            "Found 46 potential replacements from database\n",
            "\n",
            "=== SMART FILTERING FOR GROK ===\n",
            "Words detected from database matching: 220\n",
            "  Skipping '×¨×•×—' - traditional\n",
            "  Skipping '××•×¨' - traditional\n",
            "  Skipping '×—×›××”' - traditional\n",
            "  Skipping '×¦×“×§' - traditional\n",
            "  Skipping '×—×¡×“' - traditional\n",
            "  ... and 1 more skipped words\n",
            "Words in text: 631\n",
            "Words to process with Grok: 625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing with Grok: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:38<00:00, 12.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Filtered out Grok suggestion: × ×¤×© (traditional)\n",
            "\n",
            "=== GROK VALIDATION PASS ===\n",
            "Validating 146 initial suggestions...\n",
            "Sending validation request to Grok...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received validation response, parsing...\n",
            "\n",
            "ğŸ“Š VALIDATION RESULTS:\n",
            "Original suggestions: 146\n",
            "Validated suggestions: 84\n",
            "Rejected suggestions: 62\n",
            "Rejection rate: 42.5%\n",
            "Grok provided 84 additional suggestions\n",
            "Applying 46 database matches...\n",
            "Applying 84 Grok suggestions...\n",
            "  â­ï¸ Skipping ××©×™××ª ×”×—×™×™× (not found or already replaced)\n",
            "  â­ï¸ Skipping ××¨×“×£ (not found or already replaced)\n",
            "  â­ï¸ Skipping ××××¥ (not found or already replaced)\n",
            "  â­ï¸ Skipping ×“×¤×•×¡×™× (not found or already replaced)\n",
            "  â­ï¸ Skipping ××—×•×™×‘×•×™×•×ª (not found or already replaced)\n",
            "  â­ï¸ Skipping ×”×ª×¤×–×¨ (not found or already replaced)\n",
            "  â­ï¸ Skipping ×”×ª××“×” (not found or already replaced)\n",
            "  â­ï¸ Skipping ×¨××™×™×” (not found or already replaced)\n",
            "  â­ï¸ Skipping ×ª×•×‘× ×” (not found or already replaced)\n",
            "  â­ï¸ Skipping ×—×•×¡×¨ ××¢×© (not found or already replaced)\n",
            "  â­ï¸ Skipping ×›××‘ × ×¤×©×™ (not found or already replaced)\n",
            "  â­ï¸ Skipping ×—×•×•×™×” (not found or already replaced)\n",
            "  â­ï¸ Skipping ×©×¢××•× (not found or already replaced)\n",
            "  â­ï¸ Skipping ×©×•××¤×ª (not found or already replaced)\n",
            "  â­ï¸ Skipping ×”×©×¤×œ×” (not found or already replaced)\n",
            "  â­ï¸ Skipping ×œ×”×ª××™×“ (not found or already replaced)\n",
            "  â­ï¸ Skipping ×”×¢× ×§×ª ×›×‘×•×“ (not found or already replaced)\n",
            "  â­ï¸ Skipping ×ª×¢×¨×¢×¨ (not found or already replaced)\n",
            "  â­ï¸ Skipping ××§×•×¨ ×›×•×— (not found or already replaced)\n",
            "  â­ï¸ Skipping ×›×¢×¡ (not found or already replaced)\n",
            "  â­ï¸ Skipping ×ª×™×§×•×Ÿ ×”×¢×•×œ× (not found or already replaced)\n",
            "  â­ï¸ Skipping ×¦××™×—×” (not found or already replaced)\n",
            "  â­ï¸ Skipping ××›×©×•×œ (not found or already replaced)\n",
            "  â­ï¸ Skipping ××œ×—××•×ª ×©×•×•× (not found or already replaced)\n",
            "  â­ï¸ Skipping ××¢×™×™×Ÿ × ×•×‘×¢ (not found or already replaced)\n",
            "  Skipping '×¨×•×—' - traditional\n",
            "  Skipping '××•×¨' - traditional\n",
            "  Skipping '×—×›××”' - traditional\n",
            "  Skipping '×¦×“×§' - traditional\n",
            "  Skipping '×—×¡×“' - traditional\n",
            "  ... and 1 more skipped words\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_08236e0e-f47b-4a73-9ed9-133e12eaba70\", \"enhanced_processed.txt\", 18633)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6c782789-bd4d-4df8-9989-0b8de423daee\", \"analysis_report.txt\", 2666)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ‰ PROCESSING COMPLETE! ğŸ‰\n",
            "ğŸ“Š Applied 65 total replacements\n",
            "ğŸ—„ï¸ Database replacements: 7\n",
            "ğŸ¤– Grok replacements: 58\n",
            "ğŸ“ Downloaded files:\n",
            "   - enhanced_processed.txt (main output)\n",
            "   - analysis_report.txt (detailed analysis)\n",
            "   - replacement_log.json (all replacements)\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#code DUMP\n"
      ],
      "metadata": {
        "id": "2KRttHw41Ig4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "    # def enhanced_grok_api_call(self, text: str, known_words: List[str]) -> Dict[str, str]:\n",
        "    #     \"\"\"FIXED: Grok API call that actually uses the filtered words\"\"\"\n",
        "    #     if not self.grok_api_key:\n",
        "    #         return {}\n",
        "\n",
        "    #     print(f\"\\n=== SMART FILTERING FOR GROK ===\")\n",
        "    #     print(f\"Words detected from database matching: {len(known_words)}\")\n",
        "\n",
        "    #     # Extract words that appear in the text for filtering\n",
        "    #     text_words = re.findall(r'[\\u0590-\\u05FF]+', text)  # Extract Hebrew words\n",
        "    #     unique_text_words = list(set(text_words))\n",
        "\n",
        "    #     # Filter words to only those worth sending to Grok\n",
        "    #     filtered_words = self.word_filter.filter_words_for_grok_processing(unique_text_words)\n",
        "\n",
        "    #     print(f\"Words in text: {len(unique_text_words)}\")\n",
        "    #     print(f\"Words to process with Grok: {len(filtered_words)}\")\n",
        "    #     print(f\"API calls saved by filtering: {len(unique_text_words) - len(filtered_words)}\")\n",
        "\n",
        "    #     if not filtered_words:\n",
        "    #         print(\"No words need Grok processing - all are either untranslatable or traditional!\")\n",
        "    #         return {}\n",
        "\n",
        "    #     # Create cache key for this specific set of filtered words\n",
        "    #     words_hash = hashlib.md5(str(sorted(filtered_words)).encode()).hexdigest()\n",
        "\n",
        "    #     # Check cache first\n",
        "    #     if words_hash in self.processed_cache:\n",
        "    #         print(\"Using cached results for these filtered words\")\n",
        "    #         return self.processed_cache[words_hash]\n",
        "\n",
        "    #     print(f\"Sending {len(filtered_words)} filtered words to Grok for analysis...\")\n",
        "\n",
        "    #     # FIXED: Process filtered words in chunks instead of entire text\n",
        "    #     word_chunks = self.chunk_words_for_grok(filtered_words, max_words_per_chunk=250)\n",
        "\n",
        "    #     all_suggestions = {}\n",
        "\n",
        "    #     for i, word_chunk in enumerate(tqdm(word_chunks, desc=\"Processing word chunks with Grok\")):\n",
        "    #         print(f\"Processing word chunk {i+1}/{len(word_chunks)}: {word_chunk}\")\n",
        "\n",
        "    #         # FIXED: Better prompt focusing on specific filtered words\n",
        "    #         sample_untranslatable = list(self.word_filter.untranslatable_words)[:10]\n",
        "    #         sample_traditional = list(self.word_filter.already_traditional)[:8]\n",
        "\n",
        "    #         prompt = f\"\"\"\n",
        "    #         You are an expert in Hebrew linguistics specializing in Lashon Hakodesh (Traditional Hebrew).\n",
        "\n",
        "    #         TASK: For each modern Hebrew word in the list below, provide its traditional Hebrew equivalent ONLY if it has one.\n",
        "    #         the workds are extracted from sentences so you can take sentence context into account.\n",
        "\n",
        "    #         WORDS TO ANALYZE: {word_chunk}\n",
        "\n",
        "    #         CRITICAL EXCLUSIONS - DO NOT suggest equivalents for:\n",
        "    #         âŒ Modern technology: {sample_untranslatable}\n",
        "    #         âŒ Already traditional terms: {sample_traditional}\n",
        "    #         âŒ Words already in our database: {known_words[:30]}\n",
        "\n",
        "    #         INCLUDE ONLY words that are:\n",
        "    #         âœ“ Modern Hebrew neologisms with clear traditional equivalents\n",
        "    #         âœ“ Modern psychological/philosophical terms â†’ traditional concepts\n",
        "    #         âœ“ Modern abstract concepts â†’ traditional Hebrew terms\n",
        "\n",
        "    #         EXAMPLES:\n",
        "    #         âœ“ \"×¤×—×“×™×\" â†’ \"×™×¨××”\" (modern fears â†’ traditional awe/fear)\n",
        "    #         âœ“ \"××”×•×ª\" â†’ \"×¢×¦×\" (modern essence â†’ traditional essence)\n",
        "    #         âœ“ \"××ª×’×¨\" â†’ \"× ×¡×™×•×Ÿ\" (modern challenge â†’ traditional test/trial)\n",
        "\n",
        "    #         RESPONSE FORMAT:\n",
        "    #         Return ONLY valid JSON with words that have traditional equivalents:\n",
        "    #         {{\"modern_word\": \"traditional_equivalent\"}}\n",
        "\n",
        "    #         If no words have traditional equivalents, return: {{}}\n",
        "    #         \"\"\"\n",
        "\n",
        "    #         try:\n",
        "    #             headers = {\n",
        "    #                 'Content-Type': 'application/json',\n",
        "    #                 'Authorization': f'Bearer {self.grok_api_key}'\n",
        "    #             }\n",
        "\n",
        "    #             data = {\n",
        "    #                 'model': 'grok-4-latest',  # Updated model name\n",
        "    #                 'messages': [{'role': 'user', 'content': prompt}],\n",
        "    #                 'max_tokens': 3500,  # Reduced since we're sending fewer words\n",
        "    #                 'temperature': 0.1   # Very low temperature for consistency\n",
        "    #             }\n",
        "\n",
        "    #             response = requests.post(\n",
        "    #                 'https://api.x.ai/v1/chat/completions',\n",
        "    #                 headers=headers,\n",
        "    #                 json=data,\n",
        "    #                 timeout=30\n",
        "    #             )\n",
        "\n",
        "    #             if response.status_code == 200:\n",
        "    #                 result = response.json()\n",
        "    #                 content = result['choices'][0]['message']['content']\n",
        "\n",
        "    #                 # Extract JSON from response\n",
        "    #                 json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
        "    #                 json_matches = re.findall(json_pattern, content, re.DOTALL)\n",
        "\n",
        "    #                 for json_match in json_matches:\n",
        "    #                     try:\n",
        "    #                         chunk_suggestions = json.loads(json_match)\n",
        "    #                         if isinstance(chunk_suggestions, dict):\n",
        "    #                             # Validate that suggested words were actually in our filtered list\n",
        "    #                             # validated = self.validate_grok_suggestions_with_filter(chunk_suggestions, known_words)\n",
        "    #                             validated = self.validate_grok_suggestions_strict(\n",
        "    #                                 chunk_suggestions, known_words, word_chunk\n",
        "    #                             )\n",
        "    #                             all_suggestions.update(validated)\n",
        "    #                             print(f\"  âœ… Got {len(validated)} valid suggestions from chunk {i+1}\")\n",
        "    #                     except json.JSONDecodeError as e:\n",
        "    #                         print(f\"  âŒ JSON parsing error in chunk {i+1}: {e}\")\n",
        "    #                         continue\n",
        "\n",
        "    #             else:\n",
        "    #                 print(f\"  âŒ Grok API error for chunk {i+1}: {response.status_code}\")\n",
        "    #                 if response.status_code == 429:\n",
        "    #                     print(\"     Rate limit hit - consider adding delays\")\n",
        "\n",
        "    #         except Exception as e:\n",
        "    #             print(f\"  âŒ Error in Grok API call for chunk {i+1}: {e}\")\n",
        "    #             continue\n",
        "\n",
        "    #     # Cache results\n",
        "    #     self.processed_cache[words_hash] = all_suggestions\n",
        "    #     print(f\"Total suggestions from Grok: {len(all_suggestions)}\")\n",
        "    #     return all_suggestions"
      ],
      "metadata": {
        "id": "JByspk2NRM18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YvWNK3k4LNxr"
      }
    }
  ]
}